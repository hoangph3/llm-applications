{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yJJifpYDTt23"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda3/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.parameter import Parameter\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NWUoDAJw0Cs"
   },
   "source": [
    "# Implement LoRA layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "lIYdn1woOS1n"
   },
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super(LoRALayer, self).__init__()\n",
    "        self.A = Parameter(torch.empty(in_dim, rank))\n",
    "        nn.init.kaiming_uniform_(self.A, a=math.sqrt(5)) # follow: https://github.com/microsoft/LoRA/blob/main/loralib/layers.py#L124\n",
    "        self.B = Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.alpha * (x @ self.A @ self.B)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LinearWithLoRA(nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super(LinearWithLoRA, self).__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTN1SmlLxCxA"
   },
   "source": [
    "# Function to replace `Linear` layers with `LinearWithLoRA` layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "98wSOk7PxCBn"
   },
   "outputs": [],
   "source": [
    "def replace_linear_with_lora(model, rank, alpha):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Check if the module is an instance of torch.nn.Linear\n",
    "            # Replace the Linear layer with a LinearWithLoRA layer\n",
    "            setattr(model, name, LinearWithLoRA(module, rank, alpha))\n",
    "        else:\n",
    "            # If the module is not a Linear layer, recursively apply the function to the child modules\n",
    "            replace_linear_with_lora(module, rank, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ddttw6Cxkep"
   },
   "source": [
    "# Apply LoRA Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3v2o6GuhyIF9",
    "outputId": "0efc3031-ebdc-40c9-afe3-08110944d2aa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda3/envs/llm/lib/python3.9/site-packages/urllib3/connectionpool.py:1063: InsecureRequestWarning: Unverified HTTPS request is being made to host '192.168.5.8'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/home/hoang/miniconda3/envs/llm/lib/python3.9/site-packages/urllib3/connectionpool.py:1063: InsecureRequestWarning: Unverified HTTPS request is being made to host '192.168.5.8'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "import os\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "\n",
    "proxy = 'http://192.168.5.8:3128'\n",
    "os.environ['HTTP_PROXY'] = proxy\n",
    "os.environ['HTTPS_PROXY'] = proxy\n",
    "\n",
    "model = AutoModel.from_pretrained('microsoft/deberta-v3-large', cache_dir='../cache/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQYPhgpeymiA"
   },
   "source": [
    "## Calculate total number of trainable parameters of the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nq1Y1an4yX49",
    "outputId": "95a06e9a-db5b-49ea-ce70-543beb475040"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "434012160"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params_no_lora = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params_no_lora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNUD__nSytgz"
   },
   "source": [
    "## Freeze All Layers Before Applying LoRA Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "wrInm1yUyO1v"
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rNa1ewemy-7_",
    "outputId": "edaff5c5-0d06-4668-a47e-842ee601bb51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_trainable_params_after_freezing = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_trainable_params_after_freezing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArlRXjSqza2N"
   },
   "source": [
    "## Replace Linear Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "vDKWrW40zZBV"
   },
   "outputs": [],
   "source": [
    "replace_linear_with_lora(model=model, rank=8, alpha=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uE3utjFJzr5A",
    "outputId": "72899655-86cb-4e88-cf83-b17790f2f6a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3538944"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params_with_lora = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params_with_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MlPR7LMxztIJ",
    "outputId": "b244accb-ea3c-45e7-831f-5847dd57982f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.1845979614949"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((total_params_no_lora - total_params_with_lora) / total_params_no_lora) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9ZiLfG_0sQa"
   },
   "source": [
    "Thus, applying LoRA reduces the number of trainable parameters by approximately 99.18%.\n",
    "\n",
    "By applying LoRA, we are able to train the model with only about 1% of the original number of parameters. This massive reduction in the number of trainable parameters (approximately 99.18%) means that we can achieve efficient fine-tuning with significantly fewer resources, making the process much more efficient in terms of computation and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "u9TOXqkNzwFy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "d2twHxPnZvO-"
   },
   "outputs": [],
   "source": [
    "# !pip install sentence-transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ztM9i-TZaA6I"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import ast\n",
    "import gc\n",
    "import random\n",
    "import warnings\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoConfig,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_oLajjPc0OE"
   },
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0fkNAzkbbAl_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['anchor', 'positive'],\n",
      "        num_rows: 314315\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['anchor', 'positive'],\n",
      "        num_rows: 6808\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['anchor', 'positive'],\n",
      "        num_rows: 6831\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"sentence-transformers/all-nli\", \"pair\", cache_dir=\"../cache\")\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anchor</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Two women are embracing while holding to go pa...</td>\n",
       "      <td>Two woman are holding packages.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Two young children in blue jerseys, one with t...</td>\n",
       "      <td>Two kids in numbered jerseys wash their hands.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A man selling donuts to a customer during a wo...</td>\n",
       "      <td>A man selling donuts to a customer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Two young boys of opposing teams play football...</td>\n",
       "      <td>boys play football</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A man in a blue shirt standing in front of a g...</td>\n",
       "      <td>A man is wearing a blue shirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6803</th>\n",
       "      <td>Under Ferdinand and Isabella, Spain underwent ...</td>\n",
       "      <td>Ferdinand and Isabella caused stunning changes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6804</th>\n",
       "      <td>Kyoto's kabuki troupe performs in December and...</td>\n",
       "      <td>Kyoto has a kabuki troupe and so does Osaka.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6805</th>\n",
       "      <td>7) Nonautomated First-Class and Standard-A mai...</td>\n",
       "      <td>Nonautomated First-Class and Standard-A mailer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6806</th>\n",
       "      <td>Finally, the FDA will conduct workshops, issue...</td>\n",
       "      <td>The FDA is set to conduct workshops.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6807</th>\n",
       "      <td>Cirque du Soleil's The latest from the acclaim...</td>\n",
       "      <td>Cirque du Soleil is an international troupe.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6808 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 anchor  \\\n",
       "0     Two women are embracing while holding to go pa...   \n",
       "1     Two young children in blue jerseys, one with t...   \n",
       "2     A man selling donuts to a customer during a wo...   \n",
       "3     Two young boys of opposing teams play football...   \n",
       "4     A man in a blue shirt standing in front of a g...   \n",
       "...                                                 ...   \n",
       "6803  Under Ferdinand and Isabella, Spain underwent ...   \n",
       "6804  Kyoto's kabuki troupe performs in December and...   \n",
       "6805  7) Nonautomated First-Class and Standard-A mai...   \n",
       "6806  Finally, the FDA will conduct workshops, issue...   \n",
       "6807  Cirque du Soleil's The latest from the acclaim...   \n",
       "\n",
       "                                               positive  \n",
       "0                       Two woman are holding packages.  \n",
       "1        Two kids in numbered jerseys wash their hands.  \n",
       "2                   A man selling donuts to a customer.  \n",
       "3                                    boys play football  \n",
       "4                         A man is wearing a blue shirt  \n",
       "...                                                 ...  \n",
       "6803  Ferdinand and Isabella caused stunning changes...  \n",
       "6804       Kyoto has a kabuki troupe and so does Osaka.  \n",
       "6805  Nonautomated First-Class and Standard-A mailer...  \n",
       "6806               The FDA is set to conduct workshops.  \n",
       "6807       Cirque du Soleil is an international troupe.  \n",
       "\n",
       "[6808 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = ds['train']\n",
    "dev = ds['dev']\n",
    "test = ds['test']\n",
    "\n",
    "dev.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_f_-N0Zc16D"
   },
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "H_kmG2RNHUYO"
   },
   "outputs": [],
   "source": [
    "def calc_top_k_accuracy(top_k_preds, true_labels):\n",
    "    binary_label_masks = []\n",
    "    for top_k_pred, true_label in zip(top_k_preds, true_labels):\n",
    "        if true_label in top_k_pred:\n",
    "            binary_label_masks.append(1)\n",
    "        else:\n",
    "            binary_label_masks.append(0)\n",
    "    accuracy = np.mean(binary_label_masks)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nuhlmagYcoEr",
    "outputId": "22dbeb2e-1732-48bf-d394-df4489fc38d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> BEFORE FINETUNE <=====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5408a789a3b94e54b445ec16bf480ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/426 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6808, 384])\n",
      "### intfloat/multilingual-e5-small\n",
      "Accuracy@1: 0.6429200940070505\n",
      "Accuracy@3: 0.8043478260869565\n",
      "Accuracy@5: 0.8425381903642774\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f48b8eb56274c2db1d2f3287b0a1dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/426 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6808, 768])\n",
      "### intfloat/multilingual-e5-base\n",
      "Accuracy@1: 0.6527614571092832\n",
      "Accuracy@3: 0.814042303172738\n",
      "Accuracy@5: 0.8534077555816686\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_paths = [\n",
    "    'intfloat/multilingual-e5-small',\n",
    "    'intfloat/multilingual-e5-base',\n",
    "    # 'intfloat/multilingual-e5-large',\n",
    "    # 'bkai-foundation-models/vietnamese-bi-encoder',\n",
    "    # 'VoVanPhuc/sup-SimCSE-VietNamese-phobert-base'\n",
    "]\n",
    "\n",
    "print(\"=====> BEFORE FINETUNE <=====\")\n",
    "for model_path in model_paths:\n",
    "    embed_model = SentenceTransformer(model_path, cache_folder=\"../cache\")\n",
    "\n",
    "    desc_embed = embed_model.encode(\n",
    "        dev['anchor'],\n",
    "        batch_size=16,\n",
    "        device='cuda',\n",
    "        convert_to_tensor=True,\n",
    "        normalize_embeddings=True,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    print(desc_embed.shape)\n",
    "\n",
    "    top_1_preds = []\n",
    "    top_3_preds = []\n",
    "    top_5_preds = []\n",
    "    labels = []\n",
    "    for idx, query in enumerate(dev['positive']):\n",
    "        question_embed = embed_model.encode(query, device='cuda', convert_to_tensor=True, normalize_embeddings=True)\n",
    "        scores = question_embed @ desc_embed.T\n",
    "        # print(scores.shape)\n",
    "        top_1_pred = scores.argmax(dim=-1).cpu().numpy().tolist()\n",
    "        top_3_pred = scores.topk(3).indices.cpu().tolist()\n",
    "        top_5_pred = scores.topk(5).indices.cpu().tolist()\n",
    "        \n",
    "        top_1_preds.append(top_1_pred)\n",
    "        top_3_preds.append(top_3_pred)\n",
    "        top_5_preds.append(top_5_pred)\n",
    "        labels.append(idx)\n",
    "\n",
    "        # print(idx, top_1_pred, top_3_pred, top_5_pred)\n",
    "        # if idx > 100:\n",
    "        #     break\n",
    "\n",
    "    top_1_acc = accuracy_score(top_1_preds, labels)\n",
    "    top_3_acc = calc_top_k_accuracy(top_3_preds, labels)\n",
    "    top_5_acc = calc_top_k_accuracy(top_5_preds, labels)\n",
    "\n",
    "    print(f'### {model_path}')\n",
    "    print(f'Accuracy@1: {top_1_acc}')\n",
    "    print(f'Accuracy@3: {top_3_acc}')\n",
    "    print(f'Accuracy@5: {top_5_acc}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BB_On9x9IN3R"
   },
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-y1OKxMOJnnE"
   },
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'model_name': 'intfloat/multilingual-e5-small',\n",
    "    'batch_size': 32,\n",
    "    'max_length': 512,\n",
    "    'epochs': 10,\n",
    "    'learning_rate': 2e-4,\n",
    "    'warmup_steps': 0,\n",
    "    'weight_decay': 0.1,\n",
    "    'intermediate_dropout': 0.,\n",
    "    'num_workers': mp.cpu_count(),\n",
    "    'seed': 42,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}\n",
    "\n",
    "cfg = SimpleNamespace(**cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BAd3NZChLn4h"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_name, cache_dir='../cache/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     0,  32964,  24793,  ...,      1,      1,      1],\n",
      "        [     0,  32964,  27150,  ...,      1,      1,      1],\n",
      "        [     0,     62,    332,  ...,      1,      1,      1],\n",
      "        ...,\n",
      "        [     0,  49413,   3775,  ...,      1,      1,      1],\n",
      "        [     0, 201106,      4,  ...,      1,      1,      1],\n",
      "        [     0, 114765,    944,  ...,      1,      1,      1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "torch.Size([6808, 512])\n"
     ]
    }
   ],
   "source": [
    "encodings = tokenizer(\n",
    "    dev['anchor'],\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=cfg.max_length,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(encodings)\n",
    "print(encodings.input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uzWJR576E8IX"
   },
   "outputs": [],
   "source": [
    "class EmbedDataset(Dataset):\n",
    "\tdef __init__(self, encodings_1, encodings_2):\n",
    "\t\tself.encodings_1 = encodings_1\n",
    "\t\tself.encodings_2 = encodings_2\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\titem = {f'{key}_1': torch.tensor(val[idx]) for key, val in self.encodings_1.items()}\n",
    "\t\titem.update(\n",
    "            {f'{key}_2': torch.tensor(val[idx]) for key, val in self.encodings_2.items()}\n",
    "        )\n",
    "\t\treturn item\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.encodings_1.input_ids.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "l39uzuqaHkxH"
   },
   "outputs": [],
   "source": [
    "def get_dataloader(tokenizer, questions, descriptions, mode, batch_size, max_length, num_workers):\n",
    "\n",
    "\tencodings_1 = tokenizer(\n",
    "\t\tquestions,\n",
    "\t\tpadding='max_length',\n",
    "\t\ttruncation=True,\n",
    "\t\tmax_length=max_length,\n",
    "\t\treturn_tensors='pt'\n",
    "\t)\n",
    "\n",
    "\tencodings_2 = tokenizer(\n",
    "\t\tdescriptions,\n",
    "\t\tpadding='max_length',\n",
    "\t\ttruncation=True,\n",
    "\t\tmax_length=max_length,\n",
    "\t\treturn_tensors='pt'\n",
    "\t)\n",
    "\n",
    "\tdataset = EmbedDataset(encodings_1, encodings_2)\n",
    "\n",
    "\tif mode == 'train':\n",
    "\t\tdata_loader = DataLoader(\n",
    "\t\t\tdataset=dataset,\n",
    "\t\t\tbatch_size=batch_size,\n",
    "\t\t\tdrop_last=True,\n",
    "\t\t\tshuffle=True,\n",
    "\t\t\tnum_workers=num_workers\n",
    "\t\t)\n",
    "\n",
    "\telse:\n",
    "\t\tdata_loader = DataLoader(\n",
    "\t\t\tdataset=dataset,\n",
    "\t\t\tbatch_size=batch_size,\n",
    "\t\t\tdrop_last=False,\n",
    "\t\t\tshuffle=False,\n",
    "\t\t\tnum_workers=num_workers\n",
    "\t\t)\n",
    "\n",
    "\treturn data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids_1': tensor([[    0,  3493,  2412,  ...,     1,     1,     1],\n",
      "        [    0,    62, 21115,  ...,     1,     1,     1],\n",
      "        [    0, 32964, 22556,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0, 78289,  5510,  ...,     1,     1,     1],\n",
      "        [    0,    62,  4000,  ...,     1,     1,     1],\n",
      "        [    0,    17,  1600,  ...,     1,     1,     1]]), 'attention_mask_1': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'input_ids_2': tensor([[    0,   581,  3445,  ...,     1,     1,     1],\n",
      "        [    0, 41021,   621,  ...,     1,     1,     1],\n",
      "        [    0, 32964, 10269,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,  8622,   509,  ...,     1,     1,     1],\n",
      "        [    0,    62,  4000,  ...,     1,     1,     1],\n",
      "        [    0,  4263,   398,  ...,     1,     1,     1]]), 'attention_mask_2': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "torch.Size([32, 512]) torch.Size([32, 512])\n",
      "torch.Size([32, 512]) torch.Size([32, 512])\n"
     ]
    }
   ],
   "source": [
    "api_questions = dev['anchor']\n",
    "api_descriptions = dev['positive']\n",
    "\n",
    "train_dataloader = get_dataloader(\n",
    "    tokenizer=tokenizer,\n",
    "    questions=api_questions,\n",
    "    descriptions=api_descriptions,\n",
    "    mode='train',\n",
    "    batch_size=cfg.batch_size,\n",
    "    max_length=cfg.max_length,\n",
    "    num_workers=cfg.num_workers,\n",
    ")\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    anchor = batch['input_ids_1']\n",
    "    positive = batch['input_ids_2']\n",
    "    mask_1 = batch['attention_mask_1']\n",
    "    mask_2 = batch['attention_mask_2']\n",
    "    print(anchor.shape, positive.shape)\n",
    "    print(mask_1.shape, mask_2.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "JJIXtMzRSk9R"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=318):\n",
    "\trandom.seed(seed)\n",
    "\tnp.random.seed(seed)\n",
    "\tos.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\t# os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":4096:8\"\n",
    "\ttorch.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed(seed)\n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "\ttorch.backends.cudnn.benchmark = False\n",
    "\t# torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "xjoxNRh_O90X"
   },
   "outputs": [],
   "source": [
    "class MultiNegativesRankingLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Ref: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/MultipleNegativesRankingLoss.py\n",
    "    \"\"\"\n",
    "    def __init__(self, scale=50):\n",
    "        super().__init__()\n",
    "        self.cross_entropy = nn.CrossEntropyLoss(reduction='mean')\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, embed_1, embed_2, labels=None):\n",
    "        cosine_scores = (\n",
    "            F.normalize(embed_1) @ F.normalize(embed_2).T\n",
    "        ) * self.scale\n",
    "\n",
    "        labels = torch.tensor(\n",
    "            range(len(cosine_scores)),\n",
    "            dtype=torch.long,\n",
    "            device=cosine_scores.device\n",
    "        )\n",
    "\n",
    "        loss = self.cross_entropy(cosine_scores, labels)\n",
    "        return loss\n",
    "\n",
    "\n",
    "loss_fn = MultiNegativesRankingLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "-pSZtjl5Q2SK"
   },
   "outputs": [],
   "source": [
    "class TextMeanPooling(nn.Module):\n",
    "    def __init__(self, eps=1e-06):\n",
    "        super(TextMeanPooling, self).__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, token_embeddings, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        mean_embeds = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=self.eps)\n",
    "        return mean_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "iKiuZ1C3QSda"
   },
   "outputs": [],
   "source": [
    "class EmbedModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(EmbedModel, self).__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(cfg.model_name, cache_dir='../cache')\n",
    "        config.attention_probs_dropout_prob = cfg.intermediate_dropout\n",
    "        config.hidden_dropout_prob = cfg.intermediate_dropout\n",
    "\n",
    "        self.backbone = AutoModel.from_pretrained(cfg.model_name, config=config, cache_dir='../cache')\n",
    "        self.backbone.gradient_checkpointing_enable()\n",
    "\n",
    "        self.pooler = TextMeanPooling()\n",
    "        self.loss_fn = MultiNegativesRankingLoss()\n",
    "\n",
    "\n",
    "    def forward(self, input_ids_1, attention_mask_1, input_ids_2, attention_mask_2):\n",
    "        embed_1 = self.backbone(input_ids_1, attention_mask_1).last_hidden_state\n",
    "        embed_2 = self.backbone(input_ids_2, attention_mask_2).last_hidden_state\n",
    "        # print(\"EMBED\", embed_1.shape)  # (bs, max_length, embed_dim)\n",
    "\n",
    "        x_1 = self.pooler(embed_1, attention_mask_1)\n",
    "        x_2 = self.pooler(embed_2, attention_mask_2)\n",
    "        # print(\"MEAN EMBED\", x_1.shape)  # (bs, embed_dim)\n",
    "\n",
    "        loss = self.loss_fn(x_1, x_2)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(cfg.seed)\n",
    "start_time = time.time()\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Init model\n",
    "model = EmbedModel(cfg)\n",
    "model.to(cfg.device)\n",
    "model.train()\n",
    "\n",
    "# Init optim\n",
    "optimizer = optim.AdamW(model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=cfg.warmup_steps,\n",
    "    num_training_steps=len(train_dataloader)*cfg.epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BmN2G9GLSV21",
    "outputId": "a0a50bc8-2d4b-445d-854b-f48cf39bcbf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 | Batch: 0/212 | Loss: 0.1852\n",
      "Epoch: 1/10 | Batch: 10/212 | Loss: 0.0195\n",
      "Epoch: 1/10 | Batch: 20/212 | Loss: 0.0920\n",
      "Epoch: 1/10 | Batch: 30/212 | Loss: 0.1731\n",
      "Epoch: 1/10 | Batch: 40/212 | Loss: 0.2456\n",
      "Epoch: 1/10 | Batch: 50/212 | Loss: 0.3065\n",
      "Epoch: 1/10 | Batch: 60/212 | Loss: 0.1672\n",
      "Epoch: 1/10 | Batch: 70/212 | Loss: 0.0637\n",
      "Epoch: 1/10 | Batch: 80/212 | Loss: 0.1412\n",
      "Epoch: 1/10 | Batch: 90/212 | Loss: 0.1030\n",
      "Epoch: 1/10 | Batch: 100/212 | Loss: 0.1785\n",
      "Epoch: 1/10 | Batch: 110/212 | Loss: 0.1163\n",
      "Epoch: 1/10 | Batch: 120/212 | Loss: 0.1496\n",
      "Epoch: 1/10 | Batch: 130/212 | Loss: 0.0328\n",
      "Epoch: 1/10 | Batch: 140/212 | Loss: 0.0671\n",
      "Epoch: 1/10 | Batch: 150/212 | Loss: 0.0993\n",
      "Epoch: 1/10 | Batch: 160/212 | Loss: 0.1009\n",
      "Epoch: 1/10 | Batch: 170/212 | Loss: 0.0901\n",
      "Epoch: 1/10 | Batch: 180/212 | Loss: 0.0720\n",
      "Epoch: 1/10 | Batch: 190/212 | Loss: 0.1213\n",
      "Epoch: 1/10 | Batch: 200/212 | Loss: 0.1278\n",
      "Epoch: 1/10 | Batch: 210/212 | Loss: 0.0685\n",
      "Epoch: 2/10 | Batch: 0/212 | Loss: 0.0065\n",
      "Epoch: 2/10 | Batch: 10/212 | Loss: 0.0268\n",
      "Epoch: 2/10 | Batch: 20/212 | Loss: 0.0015\n",
      "Epoch: 2/10 | Batch: 30/212 | Loss: 0.0735\n",
      "Epoch: 2/10 | Batch: 40/212 | Loss: 0.2249\n",
      "Epoch: 2/10 | Batch: 50/212 | Loss: 0.0014\n",
      "Epoch: 2/10 | Batch: 60/212 | Loss: 0.0063\n",
      "Epoch: 2/10 | Batch: 70/212 | Loss: 0.1175\n",
      "Epoch: 2/10 | Batch: 80/212 | Loss: 0.0521\n",
      "Epoch: 2/10 | Batch: 90/212 | Loss: 0.0270\n",
      "Epoch: 2/10 | Batch: 100/212 | Loss: 0.1617\n",
      "Epoch: 2/10 | Batch: 110/212 | Loss: 0.0582\n",
      "Epoch: 2/10 | Batch: 120/212 | Loss: 0.1548\n",
      "Epoch: 2/10 | Batch: 130/212 | Loss: 0.1097\n",
      "Epoch: 2/10 | Batch: 140/212 | Loss: 0.0136\n",
      "Epoch: 2/10 | Batch: 150/212 | Loss: 0.0672\n",
      "Epoch: 2/10 | Batch: 160/212 | Loss: 0.1184\n",
      "Epoch: 2/10 | Batch: 170/212 | Loss: 0.0369\n",
      "Epoch: 2/10 | Batch: 180/212 | Loss: 0.0396\n",
      "Epoch: 2/10 | Batch: 190/212 | Loss: 0.1171\n",
      "Epoch: 2/10 | Batch: 200/212 | Loss: 0.0113\n",
      "Epoch: 2/10 | Batch: 210/212 | Loss: 0.0049\n",
      "Epoch: 3/10 | Batch: 0/212 | Loss: 0.0079\n",
      "Epoch: 3/10 | Batch: 10/212 | Loss: 0.0955\n",
      "Epoch: 3/10 | Batch: 20/212 | Loss: 0.0191\n",
      "Epoch: 3/10 | Batch: 30/212 | Loss: 0.0118\n",
      "Epoch: 3/10 | Batch: 40/212 | Loss: 0.0150\n",
      "Epoch: 3/10 | Batch: 50/212 | Loss: 0.0353\n",
      "Epoch: 3/10 | Batch: 60/212 | Loss: 0.0097\n",
      "Epoch: 3/10 | Batch: 70/212 | Loss: 0.0495\n",
      "Epoch: 3/10 | Batch: 80/212 | Loss: 0.0065\n",
      "Epoch: 3/10 | Batch: 90/212 | Loss: 0.0325\n",
      "Epoch: 3/10 | Batch: 100/212 | Loss: 0.0009\n",
      "Epoch: 3/10 | Batch: 110/212 | Loss: 0.0177\n",
      "Epoch: 3/10 | Batch: 120/212 | Loss: 0.0136\n",
      "Epoch: 3/10 | Batch: 130/212 | Loss: 0.0187\n",
      "Epoch: 3/10 | Batch: 140/212 | Loss: 0.0039\n",
      "Epoch: 3/10 | Batch: 150/212 | Loss: 0.1454\n",
      "Epoch: 3/10 | Batch: 160/212 | Loss: 0.0041\n",
      "Epoch: 3/10 | Batch: 170/212 | Loss: 0.0240\n",
      "Epoch: 3/10 | Batch: 180/212 | Loss: 0.0179\n",
      "Epoch: 3/10 | Batch: 190/212 | Loss: 0.0030\n",
      "Epoch: 3/10 | Batch: 200/212 | Loss: 0.0612\n",
      "Epoch: 3/10 | Batch: 210/212 | Loss: 0.0121\n",
      "Epoch: 4/10 | Batch: 0/212 | Loss: 0.0083\n",
      "Epoch: 4/10 | Batch: 10/212 | Loss: 0.1434\n",
      "Epoch: 4/10 | Batch: 20/212 | Loss: 0.0291\n",
      "Epoch: 4/10 | Batch: 30/212 | Loss: 0.0851\n",
      "Epoch: 4/10 | Batch: 40/212 | Loss: 0.0133\n",
      "Epoch: 4/10 | Batch: 50/212 | Loss: 0.1563\n",
      "Epoch: 4/10 | Batch: 60/212 | Loss: 0.0704\n",
      "Epoch: 4/10 | Batch: 70/212 | Loss: 0.0109\n",
      "Epoch: 4/10 | Batch: 80/212 | Loss: 0.0260\n",
      "Epoch: 4/10 | Batch: 90/212 | Loss: 0.0346\n",
      "Epoch: 4/10 | Batch: 100/212 | Loss: 0.0052\n",
      "Epoch: 4/10 | Batch: 110/212 | Loss: 0.0030\n",
      "Epoch: 4/10 | Batch: 120/212 | Loss: 0.0285\n",
      "Epoch: 4/10 | Batch: 130/212 | Loss: 0.0615\n",
      "Epoch: 4/10 | Batch: 140/212 | Loss: 0.0138\n",
      "Epoch: 4/10 | Batch: 150/212 | Loss: 0.0055\n",
      "Epoch: 4/10 | Batch: 160/212 | Loss: 0.0358\n",
      "Epoch: 4/10 | Batch: 170/212 | Loss: 0.0067\n",
      "Epoch: 4/10 | Batch: 180/212 | Loss: 0.0053\n",
      "Epoch: 4/10 | Batch: 190/212 | Loss: 0.3449\n",
      "Epoch: 4/10 | Batch: 200/212 | Loss: 0.1067\n",
      "Epoch: 4/10 | Batch: 210/212 | Loss: 0.0048\n",
      "Epoch: 5/10 | Batch: 0/212 | Loss: 0.0016\n",
      "Epoch: 5/10 | Batch: 10/212 | Loss: 0.0135\n",
      "Epoch: 5/10 | Batch: 20/212 | Loss: 0.0030\n",
      "Epoch: 5/10 | Batch: 30/212 | Loss: 0.0031\n",
      "Epoch: 5/10 | Batch: 40/212 | Loss: 0.0008\n",
      "Epoch: 5/10 | Batch: 50/212 | Loss: 0.1177\n",
      "Epoch: 5/10 | Batch: 60/212 | Loss: 0.0043\n",
      "Epoch: 5/10 | Batch: 70/212 | Loss: 0.0720\n",
      "Epoch: 5/10 | Batch: 80/212 | Loss: 0.0045\n",
      "Epoch: 5/10 | Batch: 90/212 | Loss: 0.0178\n",
      "Epoch: 5/10 | Batch: 100/212 | Loss: 0.0164\n",
      "Epoch: 5/10 | Batch: 110/212 | Loss: 0.0029\n",
      "Epoch: 5/10 | Batch: 120/212 | Loss: 0.0015\n",
      "Epoch: 5/10 | Batch: 130/212 | Loss: 0.0121\n",
      "Epoch: 5/10 | Batch: 140/212 | Loss: 0.0418\n",
      "Epoch: 5/10 | Batch: 150/212 | Loss: 0.0014\n",
      "Epoch: 5/10 | Batch: 160/212 | Loss: 0.0119\n",
      "Epoch: 5/10 | Batch: 170/212 | Loss: 0.0016\n",
      "Epoch: 5/10 | Batch: 180/212 | Loss: 0.0213\n",
      "Epoch: 5/10 | Batch: 190/212 | Loss: 0.0655\n",
      "Epoch: 5/10 | Batch: 200/212 | Loss: 0.0273\n",
      "Epoch: 5/10 | Batch: 210/212 | Loss: 0.0015\n",
      "Epoch: 6/10 | Batch: 0/212 | Loss: 0.0112\n",
      "Epoch: 6/10 | Batch: 10/212 | Loss: 0.0014\n",
      "Epoch: 6/10 | Batch: 20/212 | Loss: 0.0498\n",
      "Epoch: 6/10 | Batch: 30/212 | Loss: 0.0053\n",
      "Epoch: 6/10 | Batch: 40/212 | Loss: 0.0005\n",
      "Epoch: 6/10 | Batch: 50/212 | Loss: 0.0038\n",
      "Epoch: 6/10 | Batch: 60/212 | Loss: 0.0324\n",
      "Epoch: 6/10 | Batch: 70/212 | Loss: 0.0024\n",
      "Epoch: 6/10 | Batch: 80/212 | Loss: 0.0163\n",
      "Epoch: 6/10 | Batch: 90/212 | Loss: 0.0005\n",
      "Epoch: 6/10 | Batch: 100/212 | Loss: 0.0013\n",
      "Epoch: 6/10 | Batch: 110/212 | Loss: 0.0047\n",
      "Epoch: 6/10 | Batch: 120/212 | Loss: 0.0038\n",
      "Epoch: 6/10 | Batch: 130/212 | Loss: 0.0543\n",
      "Epoch: 6/10 | Batch: 140/212 | Loss: 0.0007\n",
      "Epoch: 6/10 | Batch: 150/212 | Loss: 0.0055\n",
      "Epoch: 6/10 | Batch: 160/212 | Loss: 0.0029\n",
      "Epoch: 6/10 | Batch: 170/212 | Loss: 0.0009\n",
      "Epoch: 6/10 | Batch: 180/212 | Loss: 0.0023\n",
      "Epoch: 6/10 | Batch: 190/212 | Loss: 0.0049\n",
      "Epoch: 6/10 | Batch: 200/212 | Loss: 0.0014\n",
      "Epoch: 6/10 | Batch: 210/212 | Loss: 0.0181\n",
      "Epoch: 7/10 | Batch: 0/212 | Loss: 0.0311\n",
      "Epoch: 7/10 | Batch: 10/212 | Loss: 0.0010\n",
      "Epoch: 7/10 | Batch: 20/212 | Loss: 0.0152\n",
      "Epoch: 7/10 | Batch: 30/212 | Loss: 0.0325\n",
      "Epoch: 7/10 | Batch: 40/212 | Loss: 0.0603\n",
      "Epoch: 7/10 | Batch: 50/212 | Loss: 0.0052\n",
      "Epoch: 7/10 | Batch: 60/212 | Loss: 0.0007\n",
      "Epoch: 7/10 | Batch: 70/212 | Loss: 0.0005\n",
      "Epoch: 7/10 | Batch: 80/212 | Loss: 0.0001\n",
      "Epoch: 7/10 | Batch: 90/212 | Loss: 0.0017\n",
      "Epoch: 7/10 | Batch: 100/212 | Loss: 0.0677\n",
      "Epoch: 7/10 | Batch: 110/212 | Loss: 0.0086\n",
      "Epoch: 7/10 | Batch: 120/212 | Loss: 0.0016\n",
      "Epoch: 7/10 | Batch: 130/212 | Loss: 0.0441\n",
      "Epoch: 7/10 | Batch: 140/212 | Loss: 0.0006\n",
      "Epoch: 7/10 | Batch: 150/212 | Loss: 0.0150\n",
      "Epoch: 7/10 | Batch: 160/212 | Loss: 0.0078\n",
      "Epoch: 7/10 | Batch: 170/212 | Loss: 0.0105\n",
      "Epoch: 7/10 | Batch: 180/212 | Loss: 0.0867\n",
      "Epoch: 7/10 | Batch: 190/212 | Loss: 0.0038\n",
      "Epoch: 7/10 | Batch: 200/212 | Loss: 0.0015\n",
      "Epoch: 7/10 | Batch: 210/212 | Loss: 0.0073\n",
      "Epoch: 8/10 | Batch: 0/212 | Loss: 0.0038\n",
      "Epoch: 8/10 | Batch: 10/212 | Loss: 0.0003\n",
      "Epoch: 8/10 | Batch: 20/212 | Loss: 0.0001\n",
      "Epoch: 8/10 | Batch: 30/212 | Loss: 0.0094\n",
      "Epoch: 8/10 | Batch: 40/212 | Loss: 0.0005\n",
      "Epoch: 8/10 | Batch: 50/212 | Loss: 0.0015\n",
      "Epoch: 8/10 | Batch: 60/212 | Loss: 0.0056\n",
      "Epoch: 8/10 | Batch: 70/212 | Loss: 0.0002\n",
      "Epoch: 8/10 | Batch: 80/212 | Loss: 0.0011\n",
      "Epoch: 8/10 | Batch: 90/212 | Loss: 0.1417\n",
      "Epoch: 8/10 | Batch: 100/212 | Loss: 0.0164\n",
      "Epoch: 8/10 | Batch: 110/212 | Loss: 0.0110\n",
      "Epoch: 8/10 | Batch: 120/212 | Loss: 0.0091\n",
      "Epoch: 8/10 | Batch: 130/212 | Loss: 0.0059\n",
      "Epoch: 8/10 | Batch: 140/212 | Loss: 0.0019\n",
      "Epoch: 8/10 | Batch: 150/212 | Loss: 0.0237\n",
      "Epoch: 8/10 | Batch: 160/212 | Loss: 0.0175\n",
      "Epoch: 8/10 | Batch: 170/212 | Loss: 0.0095\n",
      "Epoch: 8/10 | Batch: 180/212 | Loss: 0.0001\n",
      "Epoch: 8/10 | Batch: 190/212 | Loss: 0.0006\n",
      "Epoch: 8/10 | Batch: 200/212 | Loss: 0.0004\n",
      "Epoch: 8/10 | Batch: 210/212 | Loss: 0.0009\n",
      "Epoch: 9/10 | Batch: 0/212 | Loss: 0.0012\n",
      "Epoch: 9/10 | Batch: 10/212 | Loss: 0.0039\n",
      "Epoch: 9/10 | Batch: 20/212 | Loss: 0.0070\n",
      "Epoch: 9/10 | Batch: 30/212 | Loss: 0.0001\n",
      "Epoch: 9/10 | Batch: 40/212 | Loss: 0.0010\n",
      "Epoch: 9/10 | Batch: 50/212 | Loss: 0.0001\n",
      "Epoch: 9/10 | Batch: 60/212 | Loss: 0.0188\n",
      "Epoch: 9/10 | Batch: 70/212 | Loss: 0.0001\n",
      "Epoch: 9/10 | Batch: 80/212 | Loss: 0.0453\n",
      "Epoch: 9/10 | Batch: 90/212 | Loss: 0.0000\n",
      "Epoch: 9/10 | Batch: 100/212 | Loss: 0.0573\n",
      "Epoch: 9/10 | Batch: 110/212 | Loss: 0.0263\n",
      "Epoch: 9/10 | Batch: 120/212 | Loss: 0.0016\n",
      "Epoch: 9/10 | Batch: 130/212 | Loss: 0.0008\n",
      "Epoch: 9/10 | Batch: 140/212 | Loss: 0.0030\n",
      "Epoch: 9/10 | Batch: 150/212 | Loss: 0.0046\n",
      "Epoch: 9/10 | Batch: 160/212 | Loss: 0.0009\n",
      "Epoch: 9/10 | Batch: 170/212 | Loss: 0.0084\n",
      "Epoch: 9/10 | Batch: 180/212 | Loss: 0.0007\n",
      "Epoch: 9/10 | Batch: 190/212 | Loss: 0.0074\n",
      "Epoch: 9/10 | Batch: 200/212 | Loss: 0.0008\n",
      "Epoch: 9/10 | Batch: 210/212 | Loss: 0.0005\n",
      "Epoch: 10/10 | Batch: 0/212 | Loss: 0.0331\n",
      "Epoch: 10/10 | Batch: 10/212 | Loss: 0.0021\n",
      "Epoch: 10/10 | Batch: 20/212 | Loss: 0.0003\n",
      "Epoch: 10/10 | Batch: 30/212 | Loss: 0.0004\n",
      "Epoch: 10/10 | Batch: 40/212 | Loss: 0.0128\n",
      "Epoch: 10/10 | Batch: 50/212 | Loss: 0.0889\n",
      "Epoch: 10/10 | Batch: 60/212 | Loss: 0.0037\n",
      "Epoch: 10/10 | Batch: 70/212 | Loss: 0.0008\n",
      "Epoch: 10/10 | Batch: 80/212 | Loss: 0.0003\n",
      "Epoch: 10/10 | Batch: 90/212 | Loss: 0.0007\n",
      "Epoch: 10/10 | Batch: 100/212 | Loss: 0.0007\n",
      "Epoch: 10/10 | Batch: 110/212 | Loss: 0.0032\n",
      "Epoch: 10/10 | Batch: 120/212 | Loss: 0.0005\n",
      "Epoch: 10/10 | Batch: 130/212 | Loss: 0.0001\n",
      "Epoch: 10/10 | Batch: 140/212 | Loss: 0.0020\n",
      "Epoch: 10/10 | Batch: 150/212 | Loss: 0.0007\n",
      "Epoch: 10/10 | Batch: 160/212 | Loss: 0.0003\n",
      "Epoch: 10/10 | Batch: 170/212 | Loss: 0.0014\n",
      "Epoch: 10/10 | Batch: 180/212 | Loss: 0.0008\n",
      "Epoch: 10/10 | Batch: 190/212 | Loss: 0.0004\n",
      "Epoch: 10/10 | Batch: 200/212 | Loss: 0.0002\n",
      "Epoch: 10/10 | Batch: 210/212 | Loss: 0.0053\n"
     ]
    }
   ],
   "source": [
    "# Fit\n",
    "for epoch in range(cfg.epochs):\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        input_ids_1 = batch['input_ids_1'].to(cfg.device)\n",
    "        attention_mask_1 = batch['attention_mask_1'].to(cfg.device)\n",
    "        input_ids_2 = batch['input_ids_2'].to(cfg.device)\n",
    "        attention_mask_2 = batch['attention_mask_2'].to(cfg.device)\n",
    "\n",
    "        with autocast():\n",
    "            loss = model(\n",
    "                input_ids_1=input_ids_1,\n",
    "                attention_mask_1=attention_mask_1,\n",
    "                input_ids_2=input_ids_2,\n",
    "                attention_mask_2=attention_mask_2\n",
    "            )\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "\n",
    "        if not batch_idx % 10:\n",
    "            print(\n",
    "                f'Epoch: {epoch + 1}/{cfg.epochs}'\n",
    "                f' | Batch: {batch_idx}/{len(train_dataloader)}'\n",
    "                f' | Loss: {loss.detach().cpu().item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValDataset(Dataset):\n",
    "\tdef __init__(self, encodings):\n",
    "\t\tself.encodings = encodings\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\titem = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\t\treturn item\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.encodings['input_ids'].shape[0]\n",
    "\n",
    "\n",
    "encodings_1 = tokenizer(\n",
    "    dev['positive'],\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=cfg.max_length,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "\n",
    "val_dataset_1 = ValDataset(encodings_1)\n",
    "val_dataloader_1 = DataLoader(\n",
    "    dataset=val_dataset_1,\n",
    "    batch_size=cfg.batch_size,\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=cfg.num_workers\n",
    ")\n",
    "\n",
    "\n",
    "encodings_2 = tokenizer(\n",
    "    dev['anchor'],\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=cfg.max_length,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "\n",
    "val_dataset_2 = ValDataset(encodings_2)\n",
    "val_dataloader_2 = DataLoader(\n",
    "    dataset=val_dataset_2,\n",
    "    batch_size=cfg.batch_size,\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=cfg.num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512, 384])\n",
      "torch.Size([1, 384])\n",
      "torch.Size([6808, 384])\n"
     ]
    }
   ],
   "source": [
    "pooler = TextMeanPooling()\n",
    "desc_embed = torch.tensor([], device=cfg.device)\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for input_ids, attention_mask in zip(encodings_1.input_ids, encodings_1.attention_mask):\n",
    "        # Expand dims\n",
    "        input_ids = torch.unsqueeze(input_ids, 0).to(cfg.device)\n",
    "        attention_mask = torch.unsqueeze(attention_mask, 0).to(cfg.device)\n",
    "        # Forward\n",
    "        embed = model.backbone(input_ids, attention_mask).last_hidden_state\n",
    "        mean_embed = pooler(embed, attention_mask)\n",
    "        mean_embed = F.normalize(mean_embed, dim=-1)\n",
    "        desc_embed = torch.cat((desc_embed, mean_embed), dim=0)\n",
    "\n",
    "    print(input_ids.shape)\n",
    "    print(embed.shape)\n",
    "    print(mean_embed.shape)\n",
    "    print(desc_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> AFTER FINETUNE <=====\n",
      "### intfloat/multilingual-e5-small\n",
      "Accuracy@1: 0.7501468860164512\n",
      "Accuracy@3: 0.9225910693301997\n",
      "Accuracy@5: 0.9578437132784959\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    top_1_preds = []\n",
    "    top_3_preds = []\n",
    "    top_5_preds = []\n",
    "    labels = []\n",
    "    for idx, (input_ids, attention_mask) in enumerate(zip(encodings_2.input_ids, encodings_2.attention_mask)):\n",
    "        # Expand dims\n",
    "        input_ids = torch.unsqueeze(input_ids, 0).to(cfg.device)\n",
    "        attention_mask = torch.unsqueeze(attention_mask, 0).to(cfg.device)\n",
    "        # Forward\n",
    "        question_embed = model.backbone(input_ids, attention_mask).last_hidden_state\n",
    "        question_embed = pooler(question_embed, attention_mask)\n",
    "        question_embed = F.normalize(question_embed, dim=-1)\n",
    "        scores = question_embed @ desc_embed.T\n",
    "\n",
    "        top_1_pred = scores.argmax(dim=-1).cpu().numpy().tolist()\n",
    "        top_3_pred = scores.topk(3).indices.cpu().tolist()\n",
    "        top_5_pred = scores.topk(5).indices.cpu().tolist()\n",
    "\n",
    "        top_1_preds.extend(top_1_pred)\n",
    "        top_3_preds.extend(top_3_pred)\n",
    "        top_5_preds.extend(top_5_pred)\n",
    "        labels.append(idx)\n",
    "\n",
    "top_1_acc = accuracy_score(top_1_preds, labels)\n",
    "top_3_acc = calc_top_k_accuracy(top_3_preds, labels)\n",
    "top_5_acc = calc_top_k_accuracy(top_5_preds, labels)\n",
    "\n",
    "print(\"=====> AFTER FINETUNE <=====\")\n",
    "print(f'### {cfg.model_name}')\n",
    "print(f'Accuracy@1: {top_1_acc}')\n",
    "print(f'Accuracy@3: {top_3_acc}')\n",
    "print(f'Accuracy@5: {top_5_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

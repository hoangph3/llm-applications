{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.graphs import Neo4jGraph\n",
    "import os\n",
    "from typing import List\n",
    "from openai import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the openAI instance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "\n",
    "def embed_text(text:str)->List:\n",
    "    \"\"\"\n",
    "    Embeds the given text using the specified model.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The text to be embedded.\n",
    "\n",
    "    Returns:\n",
    "        List: A list containing the embedding of the text.\n",
    "    \"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add Neo4j credentials (These information need to be kept secret)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_URL = \"neo4j://localhost:7687\"\n",
    "NEO4J_USERNAME = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"fireinthehole\"\n",
    "NEO4J_DATABASE = 'neo4j'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Neo4jGraph(url=NEO4J_URL, username=NEO4J_USERNAME, password=NEO4J_PASSWORD, database=NEO4J_DATABASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample question for RAG:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What movies are about love?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get the questions embedding:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.004103008657693863,\n",
       " -0.030376773327589035,\n",
       " 0.0007277242839336395,\n",
       " -0.027677176520228386,\n",
       " -0.00803571566939354,\n",
       " 0.0019726611208170652,\n",
       " -0.006635457742959261,\n",
       " -0.02149585634469986,\n",
       " -0.0020720036700367928,\n",
       " -0.007903259247541428]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_embedding = embed_text(question)\n",
    "question_embedding[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform Similarity Search using the question's embedding on the vector index of the graph database and get the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'movie.title': 'Heat',\n",
       "  'movie.tagline': 'A Los Angeles crime saga',\n",
       "  'score': 0.8907528519630432},\n",
       " {'movie.title': 'Grumpier Old Men',\n",
       "  'movie.tagline': 'Still Yelling. Still Fighting. Still Ready for Love.',\n",
       "  'score': 0.8872848749160767},\n",
       " {'movie.title': 'Tom and Huck',\n",
       "  'movie.tagline': 'The Original Bad Boys.',\n",
       "  'score': 0.8832330107688904}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = graph.query(\"\"\"\n",
    "    with $question_embedding as question_embedding      // Use the provided question embedding as 'question_embedding'\n",
    "    CALL db.index.vector.queryNodes(                    // Call the vector index query function\n",
    "        'movie_tagline_embeddings',                     // Name of the vector index to query against\n",
    "        $top_k,                                         // Number of top results to retrieve\n",
    "        question_embedding                              // The question embedding to compare against\n",
    "        ) YIELD node AS movie, score                    // Yield each matched node and its similarity score\n",
    "    RETURN movie.title, movie.tagline, score            // Return the title, tagline, and similarity score of each movie\n",
    "    \"\"\",\n",
    "    params={\n",
    "        \"question_embedding\": question_embedding,       # Pass the question embedding as a parameter\n",
    "        \"top_k\": 3                                      # Specify the number of top results to retrieve\n",
    "    })\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pass the results to an LLM for the final answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some movies about love are:\n",
      "1. **Heat**: A Los Angeles crime saga\n",
      "2. **Grumpier Old Men**: Still Yelling. Still Fighting. Still Ready for Love.\n",
      "3. **Tom and Huck**: The Original Bad Boys.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"# Question:\\n{question}\\n\\n# Graph DB search results:\\n{result}\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": str(\n",
    "        \"You will be given the user question along with the search result of that question over a Neo4j graph database. Give the user the proper answer.\"\n",
    "    )},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "\n",
    "**Note: In this usecase, there is a higher chance of hallucination due to lack of enough evidence for the LLM to use its own judgment. The contents of the vector DB and the system role can address this issue to some extent.**\n",
    "\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second example (in one go):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some movies about adventure:\n",
      "1. Toy Story - \"The adventure takes off!\"\n",
      "2. Cutthroat Island - \"The Course Has Been Set. There Is No Turning Back. Prepare Your Weapons. Summon Your Courage. Discover the Adventure of a Lifetime!\"\n",
      "3. Tom and Huck - \"The Original Bad Boys.\"\n",
      "4. Jumanji - \"Roll the dice and unleash the excitement!\"\n"
     ]
    }
   ],
   "source": [
    "question = \"What movies are about adventure?\"\n",
    "question_embedding = embed_text(question)\n",
    "result = graph.query(\"\"\"\n",
    "    with $question_embedding as question_embedding\n",
    "    CALL db.index.vector.queryNodes(\n",
    "        'movie_tagline_embeddings', \n",
    "        $top_k, \n",
    "        question_embedding\n",
    "        ) YIELD node AS movie, score\n",
    "    RETURN movie.title, movie.tagline, score\n",
    "    \"\"\",\n",
    "    params={\n",
    "        \"question_embedding\": question_embedding,\n",
    "        \"top_k\": 5\n",
    "    })\n",
    "\n",
    "prompt = f\"# Question:\\n{question}\\n\\n# Graph DB search results:\\n{result}\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": str(\n",
    "        \"You will be given the user question along with the search result of that question over a Neo4j graph database. Give the user the proper answer.\"\n",
    "    )},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
